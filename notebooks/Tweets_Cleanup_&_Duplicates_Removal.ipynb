{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Tweets Cleanup & Duplicates Removal.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGNpP51XFWIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from gensim import corpora\n",
        "from gensim import similarities\n",
        "from gensim.matutils import jaccard\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import models\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpcfzSn3FfIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "cccf6f05-48b6-431e-d1da-032a7d1760ea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63dyWXuwFWIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_colwidth', 500)\n",
        "# tweets = pd.read_csv('../Data/tweets.csv')\n",
        "# tweets_cid = pd.read_csv('../Data/tweetsCID.csv')\n",
        "# knights_url = pd.read_csv('../Data/knight_data_urls.csv')\n",
        "\n",
        "tweets = pd.read_csv('/content/drive/My Drive/Documents/0_Columbia/9_Capstone/Data/tweets.csv')\n",
        "tweets_cid = pd.read_csv('/content/drive/My Drive/Documents/0_Columbia/9_Capstone/Data/tweetsCID.csv')\n",
        "knights_url = pd.read_csv('/content/drive/My Drive/Documents/0_Columbia/9_Capstone/Data/knight_data_urls.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO9DxlCpFWIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "8a958d2f-b31c-4814-f32c-fe045642c410"
      },
      "source": [
        "# tweets.info()\n",
        "tweets_cid.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 201873 entries, 0 to 201872\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count   Dtype \n",
            "---  ------       --------------   ----- \n",
            " 0   tweet_id     201873 non-null  int64 \n",
            " 1   tweet_text   201873 non-null  object\n",
            " 2   user_id      201873 non-null  int64 \n",
            " 3   tweet_time   201873 non-null  object\n",
            " 4   no_favs      201873 non-null  int64 \n",
            " 5   no_retweets  201873 non-null  int64 \n",
            " 6   urls         201815 non-null  object\n",
            " 7   content_id   201873 non-null  object\n",
            "dtypes: int64(4), object(4)\n",
            "memory usage: 12.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Wq71lPFWIj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "22fb0b71-0507-4c24-fb36-a25fb8ea677d"
      },
      "source": [
        "tweets_cid.groupby(['content_id'])['tweet_id'].count().sort_values(ascending=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "content_id\n",
              "3045    4135\n",
              "1775    3736\n",
              "2360    3221\n",
              "2661    2350\n",
              "2408    2312\n",
              "        ... \n",
              "2001       1\n",
              "466        1\n",
              "1840       1\n",
              "2484       1\n",
              "2483       1\n",
              "Name: tweet_id, Length: 1474, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcvNgSOUGcmE",
        "colab_type": "text"
      },
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb5Htc6KFWIq",
        "colab_type": "text"
      },
      "source": [
        "#### Drop duplicates based on same Tweet_Id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhrEL8LMFWIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_cid = tweets_cid.drop_duplicates(subset='tweet_id', keep=\"first\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzvcPnjSFWIy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e53efc53-bd55-4dd0-a786-bacce93ef878"
      },
      "source": [
        "print(f'Number of rows now: {tweets_cid.shape[0]:,}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows now: 198,487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCPceVLIFWI8",
        "colab_type": "text"
      },
      "source": [
        "#### Substituting the URLs with \\<URL> token using regex\n",
        " * Adding a field with a URL token instead of the URL and another field that has the list of URLs extracted. \n",
        " * Adding another column with preprocessed tweet text to remove punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaCDaAPqFWJB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "outputId": "3018d9ec-b5eb-4f6e-f076-8f585cfc879a"
      },
      "source": [
        "tweets_cid['tweet_text_url_token'] = \\\n",
        "tweets_cid['tweet_text'].apply(lambda x:re.sub(r'(https?://[www\\.]?\\w*\\.[\\w/$]*)', '<URL>', x))\n",
        "\n",
        "tweets_cid['tweet_urls'] = \\\n",
        "tweets_cid['tweet_text'].apply(lambda x:re.findall(r'(https?://[www\\.]?\\w*\\.[\\w/$]*)', x))\n",
        "\n",
        "tweets_cid['tweet_text_url_token_preprocessed'] = tweets_cid['tweet_text_url_token'].apply(lambda x:simple_preprocess(x))\n",
        "\n",
        "tweets_cid[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>user_id</th>\n",
              "      <th>tweet_time</th>\n",
              "      <th>no_favs</th>\n",
              "      <th>no_retweets</th>\n",
              "      <th>urls</th>\n",
              "      <th>content_id</th>\n",
              "      <th>tweet_text_url_token</th>\n",
              "      <th>tweet_urls</th>\n",
              "      <th>tweet_text_url_token_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>851485433003421701</td>\n",
              "      <td>#Trump’s border wall would be an ecological and financial disaster for the U.S. https://t.co/lGSc8X53Vg</td>\n",
              "      <td>613521891</td>\n",
              "      <td>2017-04-10T17:22:01.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.vox.com/energy-and-environment/2017/4/10/14471304/trump-border-wall-animals</td>\n",
              "      <td>2736</td>\n",
              "      <td>#Trump’s border wall would be an ecological and financial disaster for the U.S. &lt;URL&gt;</td>\n",
              "      <td>[https://t.co/lGSc8X53Vg]</td>\n",
              "      <td>[trump, border, wall, would, be, an, ecological, and, financial, disaster, for, the, url]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>851514117374398464</td>\n",
              "      <td>#Trump’s border wall would be an ecological and financial disaster for the U.S. https://t.co/v1TW467479</td>\n",
              "      <td>222481411</td>\n",
              "      <td>2017-04-10T19:16:00.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.vox.com/energy-and-environment/2017/4/10/14471304/trump-border-wall-animals</td>\n",
              "      <td>2736</td>\n",
              "      <td>#Trump’s border wall would be an ecological and financial disaster for the U.S. &lt;URL&gt;</td>\n",
              "      <td>[https://t.co/v1TW467479]</td>\n",
              "      <td>[trump, border, wall, would, be, an, ecological, and, financial, disaster, for, the, url]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>855009400670617600</td>\n",
              "      <td>The research and experts are clear: Tougher border security measures do not actually stop drug trafficking https://t.co/ClfsLDuDCO</td>\n",
              "      <td>2347049341</td>\n",
              "      <td>2017-04-20T10:45:01.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.vox.com/policy-and-politics/2017/4/19/15326286/trump-wall-opioid-epidemic</td>\n",
              "      <td>2644</td>\n",
              "      <td>The research and experts are clear: Tougher border security measures do not actually stop drug trafficking &lt;URL&gt;</td>\n",
              "      <td>[https://t.co/ClfsLDuDCO]</td>\n",
              "      <td>[the, research, and, experts, are, clear, tougher, border, security, measures, do, not, actually, stop, drug, trafficking, url]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>855009683542728704</td>\n",
              "      <td>Hmmmmmm https://t.co/y3fNx7KuJT</td>\n",
              "      <td>243580387</td>\n",
              "      <td>2017-04-20T10:46:08.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.vox.com/policy-and-politics/2017/4/19/15326286/trump-wall-opioid-epidemic</td>\n",
              "      <td>2644</td>\n",
              "      <td>Hmmmmmm &lt;URL&gt;</td>\n",
              "      <td>[https://t.co/y3fNx7KuJT]</td>\n",
              "      <td>[hmmmmmm, url]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>855011873183342592</td>\n",
              "      <td>(psst! It was never about drugs. It's always been about brown people. They holler DRUGS! to distract from their horrible racism) https://t.co/cMeFEnipLW</td>\n",
              "      <td>17889654</td>\n",
              "      <td>2017-04-20T10:54:50.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.vox.com/policy-and-politics/2017/4/19/15326286/trump-wall-opioid-epidemic</td>\n",
              "      <td>2644</td>\n",
              "      <td>(psst! It was never about drugs. It's always been about brown people. They holler DRUGS! to distract from their horrible racism) &lt;URL&gt;</td>\n",
              "      <td>[https://t.co/cMeFEnipLW]</td>\n",
              "      <td>[psst, it, was, never, about, drugs, it, always, been, about, brown, people, they, holler, drugs, to, distract, from, their, horrible, racism, url]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...                                                                                                                    tweet_text_url_token_preprocessed\n",
              "0  851485433003421701  ...                                                            [trump, border, wall, would, be, an, ecological, and, financial, disaster, for, the, url]\n",
              "1  851514117374398464  ...                                                            [trump, border, wall, would, be, an, ecological, and, financial, disaster, for, the, url]\n",
              "2  855009400670617600  ...                      [the, research, and, experts, are, clear, tougher, border, security, measures, do, not, actually, stop, drug, trafficking, url]\n",
              "3  855009683542728704  ...                                                                                                                                       [hmmmmmm, url]\n",
              "4  855011873183342592  ...  [psst, it, was, never, about, drugs, it, always, been, about, brown, people, they, holler, drugs, to, distract, from, their, horrible, racism, url]\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "10NvUcfzHS68"
      },
      "source": [
        "#### Drop duplicates based on same Tweet_Text after replacing the URL with the URL token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2TkQfQHG6_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "358ae099-c97f-4a92-b5fe-2ebb3c8a0b89"
      },
      "source": [
        "tweets_cid = tweets_cid.drop_duplicates(subset=['tweet_text_url_token','content_id'])\n",
        "print(f'Number of rows now: {tweets_cid.shape[0]:,}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows now: 132,159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDmLeP8qFWJG",
        "colab_type": "text"
      },
      "source": [
        "## Identify Duplicate Tweets (by Content ID) and remove them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CATqayKxFWJW",
        "colab_type": "text"
      },
      "source": [
        "### Group Tweets By Content ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7odDeaEFWJZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "14a77942-39d4-49a0-bde9-3c1ec1db9fa4"
      },
      "source": [
        "content_ids = pd.DataFrame(tweets_cid.groupby(['content_id'])['tweet_id'].count())\n",
        "content_ids.index.name = 'content_id'\n",
        "content_ids.reset_index(inplace=True)\n",
        "content_ids.rename(columns = {'tweet_id':'tweet_counts'}, inplace = True) \n",
        "content_ids = content_ids[content_ids['content_id'] !='blank']\n",
        "content_ids[:10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content_id</th>\n",
              "      <th>tweet_counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>101</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1019</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>102</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>103</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1060</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1081</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1083</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1090</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1106</td>\n",
              "      <td>441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1113</td>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  content_id  tweet_counts\n",
              "0        101            29\n",
              "1       1019            13\n",
              "2        102            15\n",
              "3        103            15\n",
              "4       1060             2\n",
              "5       1081            45\n",
              "6       1083            16\n",
              "7       1090             1\n",
              "8       1106           441\n",
              "9       1113           128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdAh6lA-FWJi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "outputId": "fe0da2d6-715f-47bb-9076-6899a411fedc"
      },
      "source": [
        "tweets_cid[tweets_cid['content_id']== '101'][:5]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>user_id</th>\n",
              "      <th>tweet_time</th>\n",
              "      <th>no_favs</th>\n",
              "      <th>no_retweets</th>\n",
              "      <th>urls</th>\n",
              "      <th>content_id</th>\n",
              "      <th>tweet_text_url_token</th>\n",
              "      <th>tweet_urls</th>\n",
              "      <th>tweet_text_url_token_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3062</th>\n",
              "      <td>902182893308575744</td>\n",
              "      <td>CNBC lets Bush's disgraced FEMA director exploit hurricane to pitch privatizing National Flood Insurance Program https://t.co/S8GeC0qwiO https://t.co/5U8Q2GsqPT</td>\n",
              "      <td>13493302</td>\n",
              "      <td>2017-08-28T14:55:38.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.mediamatters.org/video/2017/08/28/cnbc-lets-bushs-disgraced-fema-director-exploit-hurricane-pitch-privatizing-national-flood-insurance/217783</td>\n",
              "      <td>101</td>\n",
              "      <td>CNBC lets Bush's disgraced FEMA director exploit hurricane to pitch privatizing National Flood Insurance Program &lt;URL&gt; &lt;URL&gt;</td>\n",
              "      <td>[https://t.co/S8GeC0qwiO, https://t.co/5U8Q2GsqPT]</td>\n",
              "      <td>[cnbc, lets, bush, disgraced, fema, director, exploit, hurricane, to, pitch, privatizing, national, flood, insurance, program, url, url]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3063</th>\n",
              "      <td>902182884932558849</td>\n",
              "      <td>They did a heckuva job turning to him for insights... 🙄 https://t.co/4VItO0vjDC</td>\n",
              "      <td>1704646470</td>\n",
              "      <td>2017-08-28T14:55:36.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.mediamatters.org/video/2017/08/28/cnbc-lets-bushs-disgraced-fema-director-exploit-hurricane-pitch-privatizing-national-flood-insurance/217783</td>\n",
              "      <td>101</td>\n",
              "      <td>They did a heckuva job turning to him for insights... 🙄 &lt;URL&gt;</td>\n",
              "      <td>[https://t.co/4VItO0vjDC]</td>\n",
              "      <td>[they, did, heckuva, job, turning, to, him, for, insights, url]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3064</th>\n",
              "      <td>902183119519965185</td>\n",
              "      <td>Idk maybe we shouldn't let the guy who let Katrina happen have a say in this one. https://t.co/YEz571bUaE</td>\n",
              "      <td>400160847</td>\n",
              "      <td>2017-08-28T14:56:32.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.mediamatters.org/video/2017/08/28/cnbc-lets-bushs-disgraced-fema-director-exploit-hurricane-pitch-privatizing-national-flood-insurance/217783</td>\n",
              "      <td>101</td>\n",
              "      <td>Idk maybe we shouldn't let the guy who let Katrina happen have a say in this one. &lt;URL&gt;</td>\n",
              "      <td>[https://t.co/YEz571bUaE]</td>\n",
              "      <td>[idk, maybe, we, shouldn, let, the, guy, who, let, katrina, happen, have, say, in, this, one, url]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3065</th>\n",
              "      <td>902183336633913345</td>\n",
              "      <td>Heckuva job, there, CNBC https://t.co/xJQ86OMTEk</td>\n",
              "      <td>874139012</td>\n",
              "      <td>2017-08-28T14:57:23.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.mediamatters.org/video/2017/08/28/cnbc-lets-bushs-disgraced-fema-director-exploit-hurricane-pitch-privatizing-national-flood-insurance/217783</td>\n",
              "      <td>101</td>\n",
              "      <td>Heckuva job, there, CNBC &lt;URL&gt;</td>\n",
              "      <td>[https://t.co/xJQ86OMTEk]</td>\n",
              "      <td>[heckuva, job, there, cnbc, url]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3066</th>\n",
              "      <td>902182264251002880</td>\n",
              "      <td>Somebody throw this man in the f***ing ocean https://t.co/SeTKAvpR4G</td>\n",
              "      <td>59788234</td>\n",
              "      <td>2017-08-28T14:53:08.000Z</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>https://www.mediamatters.org/video/2017/08/28/cnbc-lets-bushs-disgraced-fema-director-exploit-hurricane-pitch-privatizing-national-flood-insurance/217783</td>\n",
              "      <td>101</td>\n",
              "      <td>Somebody throw this man in the f***ing ocean &lt;URL&gt;</td>\n",
              "      <td>[https://t.co/SeTKAvpR4G]</td>\n",
              "      <td>[somebody, throw, this, man, in, the, ing, ocean, url]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                tweet_id  ...                                                                                                         tweet_text_url_token_preprocessed\n",
              "3062  902182893308575744  ...  [cnbc, lets, bush, disgraced, fema, director, exploit, hurricane, to, pitch, privatizing, national, flood, insurance, program, url, url]\n",
              "3063  902182884932558849  ...                                                                           [they, did, heckuva, job, turning, to, him, for, insights, url]\n",
              "3064  902183119519965185  ...                                        [idk, maybe, we, shouldn, let, the, guy, who, let, katrina, happen, have, say, in, this, one, url]\n",
              "3065  902183336633913345  ...                                                                                                          [heckuva, job, there, cnbc, url]\n",
              "3066  902182264251002880  ...                                                                                    [somebody, throw, this, man, in, the, ing, ocean, url]\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kYX13qTJAcM",
        "colab_type": "text"
      },
      "source": [
        "### Create Dictonary and Corpus using Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMv2o6CYFWJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Punctuations Not Removed\n",
        "def create_docs(source):\n",
        "    return_dict = {}\n",
        "    for idx,row in source.iterrows():\n",
        "        return_dict[row['tweet_id']] = [word for word in row['tweet_text_url_token'].lower().split()]\n",
        "        \n",
        "    return return_dict\n",
        "\n",
        "# Punctuations Removed\n",
        "def create_docs_preprocessed(source):\n",
        "    return_dict = {}\n",
        "    for idx,row in source.iterrows():\n",
        "        return_dict[row['tweet_id']] = [word for word in row['tweet_text_url_token_preprocessed']]\n",
        "        \n",
        "    return return_dict\n",
        "\n",
        "def create_dictionary_corpus(source_dict):\n",
        "    dictionary = corpora.Dictionary()\n",
        "    corpus = []\n",
        "    \n",
        "    for key, doc in source_dict.items():\n",
        "        texts = [doc]\n",
        "        dictionary.add_documents(texts)\n",
        "        corpus.append(dictionary.doc2bow(texts[0]))\n",
        "        \n",
        "    return dictionary, corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3d28jNIFWJm",
        "colab_type": "text"
      },
      "source": [
        "### Calculate Similarity Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-SDRf7NFWJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_similarity_scores(score_type, content_ids):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    score_list = []\n",
        "\n",
        "    for id in content_ids['content_id']:\n",
        "\n",
        "        tweets_per_id = tweets_cid[tweets_cid['content_id']==id]\n",
        "        tweets_per_id = tweets_per_id [['content_id', 'tweet_text_url_token', 'tweet_text_url_token_preprocessed', 'tweet_id']]\n",
        "\n",
        "    #     tweets_dict = create_docs(tweets_per_id)  ## punctuations not removed\n",
        "        tweets_dict = create_docs_preprocessed(tweets_per_id)  ## punctuations removed\n",
        "        dictionary, corpus = create_dictionary_corpus(tweets_dict)\n",
        "        \n",
        "#         print(len(tweets_dict))\n",
        "        # COSINE Similarity Score using bag of words\n",
        "        if score_type == 'cosine_bagofwords':\n",
        "            index_cosine = similarities.MatrixSimilarity(corpus)\n",
        "            sims_cosine = index_cosine[corpus]\n",
        "            score_list.append(sims_cosine)\n",
        "\n",
        "        # COSINE Similarity Score using TF-IDF representation\n",
        "        if score_type == 'cosine_tfidf':\n",
        "            tfidf = models.TfidfModel(corpus)\n",
        "            index_tfidf = similarities.MatrixSimilarity(tfidf[corpus], num_features=len(dictionary))\n",
        "            sims_tfidf = index_tfidf[tfidf[corpus]]\n",
        "            score_list.append(sims_tfidf)\n",
        "\n",
        "    print(\"---- %s seconds ----\" % (time.time() - start_time ))\n",
        "    return score_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVq37X4BFWJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# jaccard(tweets_cid['tweet_text_url_token_preprocessed'].iloc[1], tweets_cid['tweet_text_url_token_preprocessed'].iloc[2])\n",
        "# tweets_cid['tweet_text_url_token_preprocessed'].iloc[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzgTyawlFWJ3",
        "colab_type": "text"
      },
      "source": [
        "### Create Clusters Of Similar Tweets (within Content_ID) based on similarity score threshold "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo04bTs7FWJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_clusters (score_list, content_ids, threshold):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    cluster_dict = {}\n",
        "\n",
        "    for i in range(len(score_list)):\n",
        "\n",
        "        score_array = score_list[i]\n",
        "        content_id = list(content_ids['content_id'])[i]\n",
        "        # content_id = content_ids['content_id'][i]\n",
        "\n",
        "        # print (\"Index = \", i)\n",
        "        # print (\"Content_Id = \", content_id)\n",
        "        # print (score_array.shape)\n",
        "\n",
        "        all_tweets = list(tweets_cid[tweets_cid['content_id']==content_id]['tweet_id'])\n",
        "        all_tweets_text = list(tweets_cid[tweets_cid['content_id']==content_id]['tweet_text_url_token'])\n",
        "\n",
        "        clustered_tweets = []\n",
        "        unprocessed_tweets = []\n",
        "        \n",
        "        unprocessed_tweets = all_tweets\n",
        "        length = len(unprocessed_tweets)\n",
        "        \n",
        "        # print(length)\n",
        "        \n",
        "        cnt = 0\n",
        "        \n",
        "        while length > 0:\n",
        "\n",
        "        #     print (\"cluster #\", cnt)\n",
        "        #     print (length)\n",
        "        #     cnt = cnt+1\n",
        "\n",
        "            dup_tweets = []\n",
        "\n",
        "            tweet_a = unprocessed_tweets[0]\n",
        "            tweet_a_idx = all_tweets.index(tweet_a)\n",
        "            dup_tweets.append(tweet_a)\n",
        "\n",
        "            if length > 1:\n",
        "                for col in range(1, len(unprocessed_tweets)):\n",
        "\n",
        "                    tweet_b = unprocessed_tweets[col]\n",
        "                    tweet_b_idx = all_tweets.index(tweet_b)\n",
        "\n",
        "                    if score_array[tweet_a_idx][tweet_b_idx] >= threshold:\n",
        "                        dup_tweets.append(tweet_b)        \n",
        "        #             break\n",
        "            clustered_tweets.append(dup_tweets)\n",
        "\n",
        "            unprocessed_tweets = list(set(unprocessed_tweets) - set(dup_tweets))\n",
        "            length = len(unprocessed_tweets)\n",
        "\n",
        "        cluster_dict[content_id] = clustered_tweets\n",
        "    #     break\n",
        "    print(\"---- %s seconds ----\" % (time.time() - start_time ))\n",
        "    return cluster_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AybOnYcrFWKh",
        "colab_type": "text"
      },
      "source": [
        "### Remove Duplicates by pick one tweet from each cluster (within Content_Id)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRq7CPb_FWKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_dups(cluster_dict):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    no_dups_df = tweets_cid.iloc[0:0,:].copy()\n",
        "    cnt = 0\n",
        "    for i in cluster_dict :\n",
        "        # print (cnt)\n",
        "        cnt = cnt + 1\n",
        "        \n",
        "        content_id = i\n",
        "        clusters = cluster_dict[i]\n",
        "#         print(\"Content_Id = \", i)\n",
        "        \n",
        "        for j in range(len(clusters)):\n",
        "#             print(\"Cluster # \", j)\n",
        "            first_tweet_id  = clusters[j][0]\n",
        "#             print(first_tweet_id)\n",
        "            \n",
        "            row_df = tweets_cid[(tweets_cid['tweet_id']==first_tweet_id) & (tweets_cid['content_id']==content_id)]\n",
        "            no_dups_df = pd.concat([row_df, no_dups_df], ignore_index=True)\n",
        "\n",
        "#         break\n",
        "#     print(len(no_dups_df))\n",
        "    print(\"---- %s seconds ----\" % (time.time() - start_time ))\n",
        "    return no_dups_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16Hw-cuzFWKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_clusters(cluster_dict):\n",
        "    \n",
        "    for i in cluster_dict : \n",
        "        print(\"Content_Id = \", i)\n",
        "        clusters = cluster_dict[i]\n",
        "        for j in range(len(clusters)):\n",
        "            print(\"Cluster # \", j)\n",
        "            dup_cluster = clusters[j]\n",
        "            for k in range(len(dup_cluster)):\n",
        "                tweet_text = tweets_cid[tweets_cid['tweet_id']==dup_cluster[k]]['tweet_text_url_token']\n",
        "                print(\"Tweet Id = \", dup_cluster[k])\n",
        "                print(\"Tweet Text = \", tweet_text)\n",
        "        break        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ix2NaCtFWKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ## SAMPLE TEST\n",
        "# threshold = 0.60\n",
        "# content_ids_tmp = content_ids[content_ids['content_id'] == '2168']\n",
        "# # content_ids_tmp = content_ids [100:110]\n",
        "# score_list_tmp = calculate_similarity_scores('cosine_bagofwords', content_ids_tmp)\n",
        "# # print (score_list_tmp[0].shape)\n",
        "# cosine_bow_cluster_dict = create_clusters(score_list_tmp, content_ids_tmp, threshold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDD42vC2KZf_",
        "colab_type": "text"
      },
      "source": [
        "## COSINE Similarity using bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AePFXhr5FWKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "1d8ef9f9-e481-4460-ac15-37b589e79be9"
      },
      "source": [
        "cosine_bow_threshold = 0.60\n",
        "cosine_bow_content_ids = content_ids\n",
        "# cosine_bow_content_ids = content_ids[content_ids['tweet_counts'] > 2000]\n",
        "# cosine_bow_content_ids = content_ids[content_ids['content_id'] == '2168']\n",
        "\n",
        "print(\"Calculate Similarity Score\")\n",
        "cosine_bow_score_list   = calculate_similarity_scores('cosine_bagofwords', cosine_bow_content_ids)\n",
        "\n",
        "print(\"Create Clusters\")\n",
        "cosine_bow_cluster_dict = create_clusters(cosine_bow_score_list, cosine_bow_content_ids, cosine_bow_threshold)\n",
        "\n",
        "print(\"Remove Duplicates\")\n",
        "tweet_cid_no_dups_cosine_bow = remove_dups(cosine_bow_cluster_dict)\n",
        "\n",
        "print(\"Original Records #\", len(tweets_cid))\n",
        "print(\"After Removing Dups #\", len(tweet_cid_no_dups_cosine_bow))\n",
        "\n",
        "tweet_cid_no_dups_cosine_bow.to_csv('/content/drive/My Drive/Documents/0_Columbia/9_Capstone/Data/tweetsCID_cosine_bow.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculate Similarity Score\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---- 67.15986394882202 seconds ----\n",
            "Create Clusters\n",
            "---- 214.47226285934448 seconds ----\n",
            "Remove Duplicates\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DJN7Z8jeKkSP"
      },
      "source": [
        "## COSINE Similarity using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlMeI_0CFWKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine_tfidf_threshold = 0.30\n",
        "cosine_tfidf_content_ids = content_ids\n",
        "# cosine_tfidf_content_ids = content_ids[content_ids['tweet_counts'] > 2000]\n",
        "# cosine_tfidf_content_ids = content_ids[content_ids['content_id'] == '2168']\n",
        "\n",
        "print(\"Calculate Similarity Score\")\n",
        "cosine_tfidf_score_list   = calculate_similarity_scores('cosine_tfidf', cosine_tfidf_content_ids)\n",
        "\n",
        "print(\"Create Clusters\")\n",
        "cosine_tfidf_cluster_dict = create_clusters(cosine_tfidf_score_list, cosine_tfidf_content_ids, cosine_tfidf_threshold)\n",
        "\n",
        "print(\"Remove Duplicates\")\n",
        "tweet_cid_no_dups_cosine_tfidf = remove_dups(cosine_tfidf_cluster_dict)\n",
        "\n",
        "print(\"Original Records #\", len(tweets_cid))\n",
        "print(\"After Removing Dups #\", len(tweet_cid_no_dups_cosine_tfidf))\n",
        "\n",
        "tweet_cid_no_dups_cosine_tfidf.to_csv('/content/drive/My Drive/Documents/0_Columbia/9_Capstone/Data/tweetsCID_cosine_tfidf.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_tvYpE1FWKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
