{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT & TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data - Latest Data Split - Blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"Data/latest/articles_train.csv\")\n",
    "data_test = pd.read_csv(\"Data/latest/articles_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>content_source_desc</th>\n",
       "      <th>content_title_clean</th>\n",
       "      <th>content_body_clean</th>\n",
       "      <th>blind_mean_rating</th>\n",
       "      <th>blind_rating_count</th>\n",
       "      <th>blind_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2932</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-11-02</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>A Tax Cut That Lifts the Economy? Opinions Are...</td>\n",
       "      <td>Yet if the House plan resolves some longstandi...</td>\n",
       "      <td>3.177778</td>\n",
       "      <td>45</td>\n",
       "      <td>[4.5, 1.5, 0.5, 4.5, 1.0, 4.0, 3.5, 3.5, 1.5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2870</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Tom Tancredo enters Colorado governor's race, ...</td>\n",
       "      <td>Former U.S. Rep. Tom Tancredo announced Tuesda...</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>16</td>\n",
       "      <td>[3.5, 4.0, 3.0, 2.5, 0.5, 3.0, 0.5, 0.5, 4.5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2869</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Panel Recommends Opioid Solutions but Puts No ...</td>\n",
       "      <td>President Trump’s bipartisan commission on th...</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>6</td>\n",
       "      <td>[5.0, 4.0, 4.5, 3.5, 2.0, 4.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2864</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Trump vows to end non merit-base immigration, ...</td>\n",
       "      <td>President Trump vowed Wednesday to scrap the f...</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>5</td>\n",
       "      <td>[2.0, 2.5, 4.0, 0.5, 1.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2868</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>WATCH: Congress Holds Hearing on Banning Abort...</td>\n",
       "      <td>Congress will hold a hearing Wednesday on a bi...</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>21</td>\n",
       "      <td>[2.0, 1.0, 0.5, 1.0, 3.5, 5.0, 2.0, 0.5, 2.5, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content_id  month  day  year        date content_source_desc  \\\n",
       "0        2932     11    2  2017  2017-11-02  The New York Times   \n",
       "1        2870     11    1  2017  2017-11-01            Fox News   \n",
       "2        2869     11    1  2017  2017-11-01  The New York Times   \n",
       "3        2864     11    1  2017  2017-11-01            Fox News   \n",
       "4        2868     11    1  2017  2017-11-01           Breitbart   \n",
       "\n",
       "                                 content_title_clean  \\\n",
       "0  A Tax Cut That Lifts the Economy? Opinions Are...   \n",
       "1  Tom Tancredo enters Colorado governor's race, ...   \n",
       "2  Panel Recommends Opioid Solutions but Puts No ...   \n",
       "3  Trump vows to end non merit-base immigration, ...   \n",
       "4  WATCH: Congress Holds Hearing on Banning Abort...   \n",
       "\n",
       "                                  content_body_clean  blind_mean_rating  \\\n",
       "0  Yet if the House plan resolves some longstandi...           3.177778   \n",
       "1  Former U.S. Rep. Tom Tancredo announced Tuesda...           2.375000   \n",
       "2   President Trump’s bipartisan commission on th...           3.916667   \n",
       "3  President Trump vowed Wednesday to scrap the f...           2.100000   \n",
       "4  Congress will hold a hearing Wednesday on a bi...           2.428571   \n",
       "\n",
       "   blind_rating_count                                      blind_ratings  \n",
       "0                  45  [4.5, 1.5, 0.5, 4.5, 1.0, 4.0, 3.5, 3.5, 1.5, ...  \n",
       "1                  16  [3.5, 4.0, 3.0, 2.5, 0.5, 3.0, 0.5, 0.5, 4.5, ...  \n",
       "2                   6                     [5.0, 4.0, 4.5, 3.5, 2.0, 4.5]  \n",
       "3                   5                          [2.0, 2.5, 4.0, 0.5, 1.5]  \n",
       "4                  21  [2.0, 1.0, 0.5, 1.0, 3.5, 5.0, 2.0, 0.5, 2.5, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data\n",
    "def clean_data(text):\n",
    "    #remove punctuation, digits, extra stuff. make lowercase\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    #lemma it - include POS tag in order to lemma it better\n",
    "    tag_map = defaultdict(lambda : wordnet.NOUN)\n",
    "    tag_map['J'] = wordnet.ADJ\n",
    "    tag_map['V'] = wordnet.VERB\n",
    "    tag_map['R'] = wordnet.ADV\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    textTokens = word_tokenize(text)\n",
    "    #remove stopwords\n",
    "    word_tokens_nostop = [w for w in textTokens if not w in stopwords.words('english')] \n",
    "    #now lemma\n",
    "    text = [lemmatizer.lemmatize(tok, tag_map[tag[0]]) for tok, tag in pos_tag(word_tokens_nostop)]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get clean body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_body_train = data_train[\"content_body_clean\"].apply(clean_data)\n",
    "y_train = data_train['blind_mean_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_body_test = data_test[\"content_body_clean\"].apply(clean_data)\n",
    "y_test = data_test['blind_mean_rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train_t, y_val = train_test_split(clean_body_train, y_train, test_size=0.15, shuffle=True, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_std = np.array(list(y_train_t.div(5)))\n",
    "y_val_std = np.array(list(y_val.div(5)))\n",
    "y_test_std = np.array(list(y_test.div(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_body = []\n",
    "articles = list(clean_body_train)\n",
    "\n",
    "for article in articles:\n",
    "    art_tokens = word_tokenize(article)\n",
    "    train_body.append(art_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 29377\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# train word2vec model\n",
    "modelw2 = gensim.models.Word2Vec(sentences=train_body, size=EMBED_DIM, window=5, workers=4, min_count=1)\n",
    "# vocab size\n",
    "total_words = list(modelw2.wv.vocab)\n",
    "print(\"Vocab size\",len(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 0.9432460069656372),\n",
       " ('realdonaldtrump', 0.9398722648620605),\n",
       " ('barack', 0.9383809566497803),\n",
       " ('crosshairs', 0.9366528391838074),\n",
       " ('jinping', 0.9324452877044678),\n",
       " ('obama', 0.9309636950492859),\n",
       " ('emmanuel', 0.9299267530441284),\n",
       " ('mr', 0.9168338775634766),\n",
       " ('eldest', 0.916319727897644),\n",
       " ('sought', 0.9155704975128174)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelw2.wv.most_similar('president')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('faithfully', 0.9857660531997681),\n",
       " ('intensive', 0.9810300469398499),\n",
       " ('stafford', 0.969285249710083),\n",
       " ('act', 0.9682179689407349),\n",
       " ('affordable', 0.9674921035766602),\n",
       " ('nanoscale', 0.9595308899879456),\n",
       " ('care', 0.9423438310623169),\n",
       " ('inhibitor', 0.9407963156700134),\n",
       " ('dirtcheap', 0.9346645474433899),\n",
       " ('public', 0.93406081199646)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelw2.wv.most_similar_cosmul(positive=['health','president'], negative=['trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelw2.wv.save_word2vec_format('Data/word2vec_embed.txt',binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('Data','word2vec_embed.txt'), encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize, Sequence, Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index 29377\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 29377\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(clean_body_train)\n",
    "\n",
    "# Note, the tokenizer's word_index will not respect VOCAB_SIZE.\n",
    "# but, that parameter will be respected in later methods,\n",
    "# (for example, when you call text_to_sequences).\n",
    "# Also note that '0' is a reserved index for padding.\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Word index\", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the texts_to_sequences utility to vectorize your training, \n",
    "# validation, and test questions. \n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_val = tokenizer.texts_to_sequences(X_val)\n",
    "sequences_test = tokenizer.texts_to_sequences(clean_body_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Words in the 90 percentile: 764.6\n",
      "# of Words in the 95 percentile: 1103.6\n",
      "# of Words in the 99 percentile: 1894.2000000000035\n",
      "# of Words in the 100 percentile: 9317.0\n"
     ]
    }
   ],
   "source": [
    "train_word_lengths = []\n",
    "for w in sequences_train:\n",
    "    train_word_lengths.append(len(w))\n",
    "words_length = np.array(train_word_lengths)\n",
    "\n",
    "print(\"# of Words in the 90 percentile:\",np.percentile(words_length, 90))\n",
    "print(\"# of Words in the 95 percentile:\",np.percentile(words_length, 95))\n",
    "print(\"# of Words in the 99 percentile:\",np.percentile(words_length, 99))\n",
    "print(\"# of Words in the 100 percentile:\",np.percentile(words_length, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train = tf.keras.preprocessing.sequence.pad_sequences(sequences_train, maxlen=MAX_SEQ_LEN)\n",
    "padded_val = tf.keras.preprocessing.sequence.pad_sequences(sequences_val, maxlen=MAX_SEQ_LEN)\n",
    "padded_test = tf.keras.preprocessing.sequence.pad_sequences(sequences_test, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map embeddings to tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words,EMBED_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be 0s\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input, LSTM, GRU, Bidirectional\n",
    "#from tensorflow.keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 2000, 64)          1880192   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,954,561\n",
      "Trainable params: 74,369\n",
      "Non-trainable params: 1,880,192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[\\n    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=64, input_length=MAX_SEQ_LEN),\\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\\n    tf.keras.layers.Dense(64, activation=\"relu\"),\\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\\n]'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=num_words,\n",
    "                            output_dim=EMBED_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQ_LEN,\n",
    "                            trainable=False\n",
    "                           )\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "#model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\"\"\"[\n",
    "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=64, input_length=MAX_SEQ_LEN),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=11, verbose=0, mode='min')\n",
    "cp_save = tf.keras.callbacks.ModelCheckpoint('model-e{epoch:03d}.ckpt', \n",
    "                                             save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1105 samples, validate on 196 samples\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:Assets written to: model-e001.ckpt\\assets\n",
      "1105/1105 - 482s - loss: 0.6612 - accuracy: 0.0326 - val_loss: 0.6717 - val_accuracy: 0.0102\n",
      "Epoch 2/100\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_train, y_train_std, \n",
    "                     epochs=100, \n",
    "                     verbose=2, \n",
    "                     callbacks=[earlyStopping,cp_save],\n",
    "                     validation_data=(padded_val,y_val_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting accuracy and loss as a function of epochs to ensure no overfitting\n",
    "def plot(history):\n",
    "    # The history object contains results on the training and test\n",
    "    # sets for each epoch\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Get the number of epochs\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.plot(epochs, acc, color='blue', label='Train')\n",
    "    plt.plot(epochs, val_acc, color='orange', label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    _ = plt.figure()\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.plot(epochs, loss, color='blue', label='Train')\n",
    "    plt.plot(epochs, val_loss, color='orange', label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
