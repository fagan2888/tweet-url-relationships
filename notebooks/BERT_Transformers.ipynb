{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6203,
     "status": "ok",
     "timestamp": 1583442719054,
     "user": {
      "displayName": "Rohit Dalal",
      "photoUrl": "",
      "userId": "02019963373747909975"
     },
     "user_tz": 300
    },
    "id": "IvOAdnWa5wZ4",
    "outputId": "52205069-cde7-4961-d675-6d57f1d932d5"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install --upgrade tensorflow\n",
    "# !pip install bert\n",
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ea9JdkSE_Qt-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout,Embedding, LSTM, Bidirectional, Input, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "# Used for Huggingface Transformers\n",
    "import transformers as trans\n",
    "\n",
    "# Used for TFHub\n",
    "# import bert\n",
    "# import tensorflow_hub as hub\n",
    "# from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1583442802910,
     "user": {
      "displayName": "Rohit Dalal",
      "photoUrl": "",
      "userId": "02019963373747909975"
     },
     "user_tz": 300
    },
    "id": "TLb7ivUypfGa",
    "outputId": "450e340f-5738-41d7-c1a6-7a27849ecdc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:XLA_CPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "  local_device_protos = device_lib.list_local_devices()\n",
    "  return[x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1qIl2S3s_QuL"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVn2VrhJ_QuS"
   },
   "outputs": [],
   "source": [
    "# Read temporal split file\n",
    "latest_train_url = 'https://raw.githubusercontent.com/harish-cu/tweet-url-relationships/master/data/temp/sampling_blind/latest/articles_train.csv'\n",
    "latest_test_url  = 'https://raw.githubusercontent.com/harish-cu/tweet-url-relationships/master/data/temp/sampling_blind/latest/articles_test.csv'\n",
    "\n",
    "df_articles_train  = pd.read_csv(latest_train_url)\n",
    "df_articles_test   = pd.read_csv(latest_test_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSqX2KNQ_Qvg"
   },
   "source": [
    "## Get Pre-trained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZ8chz0arLHf"
   },
   "outputs": [],
   "source": [
    "bert_model_type = 'bert-base-uncased'\n",
    "# bert_model_type = 'bert-large-uncased' \n",
    "# bert_model_type = 'bert-base-cased' \n",
    "# bert_model_type = 'bert-large-cased'\n",
    "\n",
    "bert_tokenizer_transformer = trans.BertTokenizer.from_pretrained(bert_model_type, do_lower_case=True)\n",
    "bert_model = trans.TFBertModel.from_pretrained(bert_model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZzUM2NxVunP"
   },
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5op1cw7var1V"
   },
   "outputs": [],
   "source": [
    "def _get_inputs(df,_maxlen,tokenizer):\n",
    "    \n",
    "    sentences_tokenized = []\n",
    "    sentences_mask = []\n",
    "    # sentences_segment = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "      text_seq = row[0]\n",
    "\n",
    "      encoded_dict = tokenizer.encode_plus(text_seq, \n",
    "                                           max_length = _maxlen,\n",
    "                                           pad_to_max_length = True)\n",
    "\n",
    "      sentences_tokenized.append(encoded_dict['input_ids'])\n",
    "      sentences_mask.append(encoded_dict['attention_mask'])\n",
    "      # sentences_segment.append(encoded_dict['token_type_ids']) \n",
    "\n",
    "    # print(sentences_tokenized[0])\n",
    "    # print(sentences_mask[0])\n",
    "    # print(sentences_segment[0])\n",
    "\n",
    "    # Convert list into tensor integer arrays and return it\n",
    "    return [tf.cast(sentences_tokenized,tf.int32), \n",
    "            tf.cast(sentences_mask,tf.int32)\n",
    "            # tf.cast(sentences_segment,tf.int32)\n",
    "            ]\n",
    "\n",
    "##########################################################\n",
    "\n",
    "def _get_inputs_source(df,_maxlen,tokenizer):\n",
    "    \n",
    "    sentences_tokenized = []\n",
    "    sentences_mask = []\n",
    "    sentences_segment = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "      source_seq = row[0]\n",
    "      text_seq   = row[1]\n",
    "\n",
    "      encoded_dict = tokenizer.encode_plus(source_seq, \n",
    "                                           text_seq, \n",
    "                                           max_length = _maxlen,\n",
    "                                           pad_to_max_length = True)\n",
    "\n",
    "      sentences_tokenized.append(encoded_dict['input_ids'])\n",
    "      sentences_mask.append(encoded_dict['attention_mask'])\n",
    "      sentences_segment.append(encoded_dict['token_type_ids']) \n",
    "\n",
    "    # print(sentences_tokenized[0])\n",
    "    # print(sentences_mask[0])\n",
    "    # print(sentences_segment[0])\n",
    "\n",
    "    # Convert list into tensor integer arrays and return it\n",
    "    return [tf.cast(sentences_tokenized,tf.int32), \n",
    "            tf.cast(sentences_mask,tf.int32),\n",
    "            tf.cast(sentences_segment,tf.int32)]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vSnm9Tk_Qvu"
   },
   "source": [
    "## Model 1 (Article Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W99h3FFRYVBY"
   },
   "outputs": [],
   "source": [
    "X_train = df_articles_train[['content_body_clean']]\n",
    "X_test  = df_articles_test[['content_body_clean']]\n",
    "\n",
    "y_train = df_articles_train[['blind_mean_rating']]\n",
    "y_test  = df_articles_test[['blind_mean_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ta_kpvmc_Qvb"
   },
   "outputs": [],
   "source": [
    "max_sequence_length = 128\n",
    "\n",
    "bert_inputs_train = _get_inputs(df=X_train, tokenizer=bert_tokenizer_transformer, _maxlen=max_sequence_length)\n",
    "bert_inputs_test  = _get_inputs(df=X_test,  tokenizer=bert_tokenizer_transformer, _maxlen=max_sequence_length)\n",
    "\n",
    "Xtr = bert_inputs_train\n",
    "ytr = np.asarray(y_train)\n",
    "\n",
    "Xte = bert_inputs_test\n",
    "yte = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2652,
     "status": "ok",
     "timestamp": 1583441209488,
     "user": {
      "displayName": "Rohit Dalal",
      "photoUrl": "",
      "userId": "02019963373747909975"
     },
     "user_tz": 300
    },
    "id": "m-mBnCi-_Qvv",
    "outputId": "a883f315-44b2-40f5-8ced-b81ce1afb77e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     multiple             109482240   input_word_ids[0][0]             \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 768)          0           tf_bert_model[3][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          76900       global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,559,241\n",
      "Trainable params: 109,559,241\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs_length = max_sequence_length\n",
    "\n",
    "token_inputs = Input((inputs_length), dtype=tf.int32, name='input_word_ids')\n",
    "mask_inputs = Input((inputs_length), dtype=tf.int32, name='input_masks')\n",
    "# seg_inputs = Input((inputs_length), dtype=tf.int32, name='input_segments')\n",
    "\n",
    "# seq_output,_ = bert_model([token_inputs, mask_inputs, seg_inputs])\n",
    "seq_output,_ = bert_model([token_inputs, mask_inputs])\n",
    "X = GlobalAveragePooling1D()(seq_output)\n",
    "X = Dense(100, activation='relu')(X)\n",
    "output_= Dense(1, activation='linear', name='output')(X)\n",
    "# output_= Dense(1, activation=None, name='output')(X)\n",
    "\n",
    "# bert_model2 = Model([token_inputs, mask_inputs, seg_inputs],output_)\n",
    "bert_model2 = Model([token_inputs, mask_inputs],output_)\n",
    "bert_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spBrsdI1_Qv0"
   },
   "outputs": [],
   "source": [
    "lr = 0.0001  ## 0.1, 0.01, 0.001, 2e-5 \n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true)))\n",
    "\n",
    "bert_model2.compile(optimizer=opt, loss='mse', metrics=['mae', rmse])\n",
    " # bert_model2.compile(optimizer='Adam', loss='mse', metrics=['mae', rmse])\n",
    "\n",
    "# Callbacks\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
    "# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2KQdrv9_Qv5"
   },
   "outputs": [],
   "source": [
    "history = bert_model2.fit(Xtr, ytr, \n",
    "                          epochs=25, \n",
    "                          validation_split = 0.2,\n",
    "                          # batch_size = 100,\n",
    "                          # validation_data=(Xtr_val, ytr_val),\n",
    "                          verbose = 1,\n",
    "                          callbacks=[earlyStopping]\n",
    "                          # callbacks=[earlyStopping, reduce_lr]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCfi17Kl_Qv9"
   },
   "outputs": [],
   "source": [
    "test_results = bert_model2.evaluate(Xte, yte)\n",
    "print('Test MSE: {0:.4f}'.format(test_results[0]))\n",
    "print('Test MAE: {0:.4f}'.format(test_results[1]))\n",
    "print('Test RMSE: {0:.4f}'.format(test_results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1O9JouKzWgU"
   },
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(train_loss))\n",
    "\n",
    "plt.plot(epochs, train_loss, label='Traning Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "87bCiFPx08el"
   },
   "outputs": [],
   "source": [
    "y_pred = bert_model2.predict(Xte)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RikYroz522Px"
   },
   "outputs": [],
   "source": [
    "yte[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0G10qw4KJfT"
   },
   "outputs": [],
   "source": [
    "# y_pred = y_pred.reshape(y_pred.shape[0])\n",
    "# d = yte - y_pred\n",
    "# mse_f = np.mean(d**2)\n",
    "# mae_f = np.mean(abs(d))\n",
    "# rmse_f = np.sqrt(mse_f)\n",
    "# # r2_f = 1-(np.sum(d**2)/np.sum((y-np.mean(y))**2))\n",
    "\n",
    "# print(\"RESULTS...\")\n",
    "# print(\"MAE:\",mae_f)\n",
    "# print(\"MSE:\", mse_f)\n",
    "# print(\"RMSE:\", rmse_f)\n",
    "# print(\"R-Squared:\", r2_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IC2zYDYrclGl"
   },
   "outputs": [],
   "source": [
    "# def plot_history(histories, key='loss'):\n",
    "#   plt.figure(figsize=(10,8))\n",
    "    \n",
    "#   for name, history in histories:\n",
    "#     val = plt.plot([x+1 for x in history.epoch], history.history['val_'+key],\n",
    "#                    label='Val_'+key)\n",
    "#     plt.plot([x+1 for x in history.epoch], history.history[key],\n",
    "#              label='Train_'+key)\n",
    "\n",
    "#   plt.xlabel('Epochs')\n",
    "#   plt.xticks([x+1 for x in history.epoch])\n",
    "#   plt.ylabel(key.replace('_',' ').title())\n",
    "#   plt.legend()\n",
    "\n",
    "#  plot_history([('linear_model', history)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYvt6i-j1ySW"
   },
   "outputs": [],
   "source": [
    "### Fit on test data\n",
    "plt.plot(yte, y_pred, '.')\n",
    "\n",
    "# # plot a line, a perfet predict would all fall on this line\n",
    "x = np.linspace(0, 5)\n",
    "y = x\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rd0H9xcmZFAI"
   },
   "source": [
    "## Model 2 (Source + Article Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYqw2ypGZG6w"
   },
   "outputs": [],
   "source": [
    "X_train_source = df_articles_train[['content_source_desc','content_body_clean']]\n",
    "X_test_source  = df_articles_test[['content_source_desc','content_body_clean']]\n",
    "\n",
    "y_train = df_articles_train[['blind_mean_rating']]\n",
    "y_test  = df_articles_test[['blind_mean_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOR0kh2xZQAr"
   },
   "outputs": [],
   "source": [
    "max_sequence_length = 128\n",
    "\n",
    "bert_inputs_train_source = _get_inputs_source(df=X_train_source, tokenizer=bert_tokenizer_transformer, _maxlen=max_sequence_length)\n",
    "bert_inputs_test_source  = _get_inputs_source(df=X_test_source,  tokenizer=bert_tokenizer_transformer, _maxlen=max_sequence_length)\n",
    "\n",
    "Xtr_source = bert_inputs_train_source\n",
    "ytr = np.asarray(y_train)\n",
    "\n",
    "Xte_source = bert_inputs_test_source\n",
    "yte = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3102,
     "status": "ok",
     "timestamp": 1583442912011,
     "user": {
      "displayName": "Rohit Dalal",
      "photoUrl": "",
      "userId": "02019963373747909975"
     },
     "user_tz": 300
    },
    "id": "eL1_OLf-4Spt",
    "outputId": "6d92dc56-f8e4-4cb6-d69c-34fa5ddae245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_1 (TFBertModel)   ((None, 128, 768), ( 109482240   input_word_ids[0][0]             \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 768)          0           tf_bert_model_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 100)          76900       global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            101         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,559,241\n",
      "Trainable params: 109,559,241\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs_length = max_sequence_length\n",
    "\n",
    "token_inputs = Input((inputs_length), dtype=tf.int32, name='input_word_ids')\n",
    "mask_inputs = Input((inputs_length), dtype=tf.int32, name='input_masks')\n",
    "seg_inputs = Input((inputs_length), dtype=tf.int32, name='input_segments')\n",
    "\n",
    "seq_output,_ = bert_model([token_inputs, mask_inputs, seg_inputs])\n",
    "X = GlobalAveragePooling1D()(seq_output)\n",
    "X = Dense(100, activation='relu')(X)\n",
    "output_= Dense(1, activation='linear', name='output')(X)\n",
    "# output_= Dense(1, activation=None, name='output')(X)\n",
    "\n",
    "bert_model3 = Model([token_inputs, mask_inputs, seg_inputs],output_)\n",
    "bert_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FlE3_1mF4Sp8"
   },
   "outputs": [],
   "source": [
    "lr = 0.0001  ## 0.1, 0.01, 0.001, 2e-5 \n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true)))\n",
    "\n",
    "bert_model3.compile(optimizer=opt, loss='mse', metrics=['mae', rmse]) \n",
    "\n",
    "# Callbacks\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
    "# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "mUshybQx4SqG",
    "outputId": "6a47b6cf-54fd-421f-9d98-6d5fb937efa1"
   },
   "outputs": [],
   "source": [
    "history_source = bert_model3.fit(Xtr_source, ytr, \n",
    "                                epochs=10, \n",
    "                                validation_split = 0.2,\n",
    "                                # batch_size = 100,\n",
    "                                # validation_data=(Xtr_val, ytr_val),\n",
    "                                verbose = 1,\n",
    "                                callbacks=[earlyStopping]\n",
    "                                # callbacks=[earlyStopping, reduce_lr]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6034,
     "status": "ok",
     "timestamp": 1583428855512,
     "user": {
      "displayName": "Rohit Dalal",
      "photoUrl": "",
      "userId": "02019963373747909975"
     },
     "user_tz": 300
    },
    "id": "grl4QgjF4SqM",
    "outputId": "cd808bae-b706-4e33-d607-430360ca986a"
   },
   "outputs": [],
   "source": [
    "test_results = bert_model3.evaluate(Xte_source, yte)\n",
    "print('Test MSE: {0:.4f}'.format(test_results[0]))\n",
    "print('Test MAE: {0:.4f}'.format(test_results[1]))\n",
    "print('Test RMSE: {0:.4f}'.format(test_results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NGod0KV4SqV"
   },
   "outputs": [],
   "source": [
    "train_loss = history_source.history['loss']\n",
    "val_loss = history_source.history['val_loss']\n",
    "epochs = range(len(train_loss))\n",
    "\n",
    "plt.plot(epochs, train_loss, label='Traning Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCsRp_Im4Sqb"
   },
   "outputs": [],
   "source": [
    "y_pred = bert_model3.predict(Xte_source)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7u9YUon4Sqh"
   },
   "outputs": [],
   "source": [
    "yte[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oCjxk-LX4Sqm"
   },
   "outputs": [],
   "source": [
    "# y_pred = y_pred.reshape(y_pred.shape[0])\n",
    "# d = yte - y_pred\n",
    "# mse_f = np.mean(d**2)\n",
    "# mae_f = np.mean(abs(d))\n",
    "# rmse_f = np.sqrt(mse_f)\n",
    "# # r2_f = 1-(np.sum(d**2)/np.sum((y-np.mean(y))**2))\n",
    "\n",
    "# print(\"RESULTS...\")\n",
    "# print(\"MAE:\",mae_f)\n",
    "# print(\"MSE:\", mse_f)\n",
    "# print(\"RMSE:\", rmse_f)\n",
    "# print(\"R-Squared:\", r2_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OiRU9yok4Sqs"
   },
   "outputs": [],
   "source": [
    "# def plot_history(histories, key='loss'):\n",
    "#   plt.figure(figsize=(10,8))\n",
    "    \n",
    "#   for name, history in histories:\n",
    "#     val = plt.plot([x+1 for x in history.epoch], history.history['val_'+key],\n",
    "#                    label='Val_'+key)\n",
    "#     plt.plot([x+1 for x in history.epoch], history.history[key],\n",
    "#              label='Train_'+key)\n",
    "\n",
    "#   plt.xlabel('Epochs')\n",
    "#   plt.xticks([x+1 for x in history.epoch])\n",
    "#   plt.ylabel(key.replace('_',' ').title())\n",
    "#   plt.legend()\n",
    "\n",
    "#  plot_history([('linear_model', history)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q660X72l4Sqz"
   },
   "outputs": [],
   "source": [
    "### Fit on test data\n",
    "plt.plot(yte, y_pred, '.')\n",
    "\n",
    "# # plot a line, a perfet predict would all fall on this line\n",
    "x = np.linspace(0, 5)\n",
    "y = x\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BERT_Transformers.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
